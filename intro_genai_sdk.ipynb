{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-02-ca08c6e4f0dd\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye Sad Desk Lunch, Hello Flavorful Fuel!\n",
      "\n",
      "Tired of the midday scramble? Dreading another uninspired lunch? Or perhaps you're simply looking for a smarter way to fuel your busy week? Just look at these vibrant, perfectly portioned containers – they hold the secret to stress-free, delicious eating!\n",
      "\n",
      "These glass meal prep bowls are brimming with goodness: succulent, glazed chicken (hello, sesame seeds and green onions!), bright green broccoli florets, and crisp, colorful strips of red bell pepper and carrots, all served over fluffy rice. It's a symphony of textures and flavors that promises to keep you satisfied and energized.\n",
      "\n",
      "Meal prepping isn't just about saving time; it's about taking control of your nutrition, avoiding unhealthy impulse buys, and enjoying homemade goodness even on your busiest days. Imagine grabbing one of these beauties from your fridge, knowing you've got a balanced, delicious meal ready to go.\n",
      "\n",
      "So, why not dedicate a little time this weekend to prepare for a week of wholesome, mouth-watering meals? Your future self (and your taste buds!) will thank you. Happy prepping!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short and engaging blog post based on the image:\n",
      "\n",
      "***\n",
      "\n",
      "## Revolutionize Your Week: Vibrant & Flavorful Teriyaki Chicken Meal Prep!\n",
      "\n",
      "Tired of staring at a sad, soggy sandwich come lunchtime? Dreading another expensive and unhealthy takeout order? It's time to reclaim your lunch break, and we've got just the solution! Feast your eyes on these perfectly portioned, incredibly delicious Teriyaki Chicken Meal Prep bowls.\n",
      "\n",
      "Imagine opening your fridge each morning to find a ready-to-go, homemade meal that's not only good for you but also bursting with flavor and color. These clear glass containers aren't just pretty; they're packed with everything you need to power through your day.\n",
      "\n",
      "**What makes these bowls so irresistible?**\n",
      "\n",
      "Each container features tender, succulent chicken pieces, glistening with a savory teriyaki glaze and generously sprinkled with toasted sesame seeds and fresh green onions for an extra pop. Complementing the chicken are bright green broccoli florets, adding a satisfying crunch and a boost of essential nutrients. Alongside, you'll find vibrant strips of red bell pepper and crisp carrots, bringing a touch of sweetness and even more vitamins to the mix. All of this goodness is served over a bed of fluffy, wholesome rice – the perfect foundation for a truly balanced meal.\n",
      "\n",
      "The beauty of this meal prep isn't just in its taste; it's in its simplicity. Spend a little time in the kitchen over the weekend, and you'll have delicious, nutritious meals ready to grab and go for days. No more last-minute food decisions, no more compromises on taste or health.\n",
      "\n",
      "So, if you're ready to transform your lunch routine from drab to fab, give this Teriyaki Chicken Meal Prep a try. Your taste buds (and your schedule) will thank you!\n",
      "\n",
      "**What are your favorite meal prep combos? Share your ideas in the comments below!**\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, little fluffball, listen up! *Woof woof!*\n",
      "\n",
      "You know your favorite squeaky toy, right? The one that goes **SQUEAK!** when you boop it with your nose?\n",
      "\n",
      "Well, imagine the whole world is full of other puppies, and *they all have squeaky toys too!*\n",
      "\n",
      "1.  **You want a Squeak!**\n",
      "    *   You're sitting there, maybe with your little nosey-boop (that's your computer or phone), and you want to hear a *different* squeak. Maybe a squeak about a squirrel! Or a squeak about a yummy treat! *Arf!*\n",
      "\n",
      "2.  **You Squeeze Your Toy!**\n",
      "    *   You use your paw (your finger on the screen) to make your nosey-boop go **SQUEEZE!** That's like you saying, \"Hey! I want a squirrel squeak!\"\n",
      "\n",
      "3.  **Your Squeak Zooms Out!**\n",
      "    *   That **SQUEEZE!** sound (which is really your question) doesn't just stay in your paw. Oh no! It zooms out into the *biggest, most amazing invisible playpen ever!* This playpen is full of invisible paths, like super-duper doggy smells that only squeaks can follow.\n",
      "\n",
      "4.  **It Finds a Big Toy Box!**\n",
      "    *   Your squeak zooms and zooms, through special doggy doors (that's your Wi-Fi box!), until it finds a *really, really big toy box* (that's like a website or a server). This big toy box holds *all sorts* of squeaks! It has squirrel squeaks, treat squeaks, belly rub squeaks – everything!\n",
      "\n",
      "5.  **The Big Toy Box Squeaks Back!**\n",
      "    *   The big toy box hears your \"squirrel squeak!\" request, and it says, \"Oh! You want *that* squeak?\" And then, *WHOOSH!* It sends *its* squirrel squeak back to you!\n",
      "\n",
      "6.  **The Squeak Comes Home!**\n",
      "    *   That squirrel squeak zooms back along all those invisible paths, through your doggy door, right back to your nosey-boop! And then you hear it! **SQUEAK! SQUEAK!** The squirrel squeak! You got your squeak!\n",
      "\n",
      "So, the internet is just a giant, invisible playpen where all the puppies and their nosey-boops can send and receive **SQUEAKS!** (which are really just information) to each other, super-duper fast, even if they're far, far away!\n",
      "\n",
      "It's like playing fetch with a million squeaky toys all at once! *Good boy! Good girl!* Now go chase that tail!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"**Oh, *real* clever, Universe. Just when I thought you couldn't get any more creatively sadistic, you hit me with the furniture.**\"\n",
      "2.  \"**Is this your grand design, you incompetent cosmic architect? My toe, *again*?!**\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0011542806,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00033999846,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00615857,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.029268384\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0002159381,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.06991494\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python function that checks if a year is a leap year, along with explanations and examples.\n",
      "\n",
      "## Leap Year Rules\n",
      "\n",
      "A year is a leap year if:\n",
      "\n",
      "1.  It is divisible by 4.\n",
      "2.  **Except** if it is divisible by 100.\n",
      "3.  **Unless** it is also divisible by 400.\n",
      "\n",
      "In simpler terms:\n",
      "*   Years like 2004, 2008, 2020 are leap years (divisible by 4, not by 100).\n",
      "*   Years like 1900, 2100 are NOT leap years (divisible by 100, but not by 400).\n",
      "*   Years like 1600, 2000, 2400 ARE leap years (divisible by 400).\n",
      "\n",
      "---\n",
      "\n",
      "### Python Implementation\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a positive integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Examples:\n",
      "        >>> is_leap_year(2000)\n",
      "        True\n",
      "        >>> is_leap_year(1900)\n",
      "        False\n",
      "        >>> is_leap_year(2004)\n",
      "        True\n",
      "        >>> is_leap_year(2007)\n",
      "        False\n",
      "        >>> is_leap_year(1600)\n",
      "        True\n",
      "        >>> is_leap_year(2100)\n",
      "        False\n",
      "    \"\"\"\n",
      "    # Rule 1: Divisible by 4\n",
      "    # Rule 2: NOT divisible by 100 (unless Rule 3 applies)\n",
      "    # Rule 3: Divisible by 400\n",
      "    \n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Test Cases ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\") # Expected: True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\") # Expected: False (divisible by 100, not by 400)\n",
      "print(f\"Is 2004 a leap year? {is_leap_year(2004)}\") # Expected: True (divisible by 4, not by 100)\n",
      "print(f\"Is 2007 a leap year? {is_leap_year(2007)}\") # Expected: False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\") # Expected: True (divisible by 400)\n",
      "print(f\"Is 2100 a leap year? {is_leap_year(2100)}\") # Expected: False (divisible by 100, not by 400)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\") # Expected: True\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The core logic `(year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)` directly translates the rules:\n",
      "\n",
      "1.  `year % 4 == 0`: Checks if the year is divisible by 4.\n",
      "2.  `year % 100 != 0`: Checks if the year is NOT divisible by 100.\n",
      "    *   If both (1) and (2) are true, then it's a leap year (e.g., 2004). This covers the most common case.\n",
      "3.  `year % 400 == 0`: This is the exception to the \"divisible by 100\" rule. If a year is divisible by 400, it *is* a leap year, even if it's also divisible by 100 (e.g., 2000).\n",
      "\n",
      "The `or` operator combines these conditions: if either the first set of conditions (divisible by 4 AND not by 100) is true, OR the second condition (divisible by 400) is true, then the year is a leap year.\n",
      "\n",
      "---\n",
      "\n",
      "### Alternative (using Python's built-in `calendar` module)\n",
      "\n",
      "For practical applications in Python, you might prefer to use the `calendar` module, which already provides this functionality and handles edge cases or historical calendar changes if applicable (though for standard Gregorian leap years, the logic is the same).\n",
      "\n",
      "```python\n",
      "import calendar\n",
      "\n",
      "def is_leap_year_builtin(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year using Python's built-in calendar module.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    return calendar.isleap(year)\n",
      "\n",
      "print(\"\\n--- Using built-in calendar module ---\")\n",
      "print(f\"Is 2000 a leap year? {is_leap_year_builtin(2000)}\")\n",
      "print(f\"Is 1900 a leap year? {is_leap_year_builtin(1900)}\")\n",
      "print(f\"Is 2024 a leap year? {is_leap_year_builtin(2024)}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a unit test for the `is_leap_year` function using Python's built-in `unittest` module.\n",
      "\n",
      "First, let's make sure we have the `is_leap_year` function available. I'll include it again for completeness.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "# The function to be tested (copied from the previous response)\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a positive integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Unit Test Class ---\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "    \"\"\"\n",
      "    Unit tests for the is_leap_year function.\n",
      "    \"\"\"\n",
      "\n",
      "    def test_divisible_by_4_not_by_100(self):\n",
      "        \"\"\"\n",
      "        Test cases for years divisible by 4 but not by 100 (should be leap years).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2004), \"2004 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2008), \"2008 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year\")\n",
      "\n",
      "    def test_divisible_by_400(self):\n",
      "        \"\"\"\n",
      "        Test cases for years divisible by 400 (should be leap years).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year\")\n",
      "        self.assertTrue(is_leap_year(800), \"800 should be a leap year\")\n",
      "\n",
      "    def test_divisible_by_100_not_by_400(self):\n",
      "        \"\"\"\n",
      "        Test cases for years divisible by 100 but not by 400 (should NOT be leap years).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1700), \"1700 should NOT be a leap year\")\n",
      "\n",
      "    def test_not_divisible_by_4(self):\n",
      "        \"\"\"\n",
      "        Test cases for years not divisible by 4 (should NOT be leap years).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(2007), \"2007 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should NOT be a leap year\")\n",
      "        self.assertFalse(is_leap_year(2001), \"2001 should NOT be a leap year\")\n",
      "\n",
      "    def test_zero_and_negative_years(self):\n",
      "        \"\"\"\n",
      "        Test edge cases like zero and negative years (though not standard calendar years).\n",
      "        The current implementation treats them based on the modulo logic.\n",
      "        \"\"\"\n",
      "        # Year 0: Divisible by 4, 100, and 400. Should technically be a leap year by the rule.\n",
      "        self.assertTrue(is_leap_year(0), \"Year 0 should be a leap year by logic\")\n",
      "        # Negative years: -4 is divisible by 4, not 100.\n",
      "        self.assertTrue(is_leap_year(-4), \"-4 should be a leap year by logic\")\n",
      "        # Negative years: -100 is divisible by 100, not 400.\n",
      "        self.assertFalse(is_leap_year(-100), \"-100 should NOT be a leap year by logic\")\n",
      "        # Negative years: -400 is divisible by 400.\n",
      "        self.assertTrue(is_leap_year(-400), \"-400 should be a leap year by logic\")\n",
      "        # Negative years: -7 is not divisible by 4.\n",
      "        self.assertFalse(is_leap_year(-7), \"-7 should NOT be a leap year by logic\")\n",
      "\n",
      "\n",
      "# This allows you to run the tests directly from the script\n",
      "if __name__ == '__main__':\n",
      "    # unittest.main() runs all tests in the current file.\n",
      "    # argv=['first-arg-is-ignored'] and exit=False are often used\n",
      "    # when running tests in environments like Jupyter notebooks or some IDEs\n",
      "    # to prevent issues with argument parsing or exiting the interpreter.\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "```\n",
      "\n",
      "### How to Run This Test:\n",
      "\n",
      "1.  **Save:** Save the code above as a Python file (e.g., `test_leap_year.py`).\n",
      "2.  **Execute:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run:\n",
      "    ```bash\n",
      "    python -m unittest test_leap_year.py\n",
      "    ```\n",
      "    or simply:\n",
      "    ```bash\n",
      "    python test_leap_year.py\n",
      "    ```\n",
      "\n",
      "### Expected Output:\n",
      "\n",
      "If all tests pass, you should see output similar to this:\n",
      "\n",
      "```\n",
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.000s\n",
      "\n",
      "OK\n",
      "```\n",
      "\n",
      "(The number of dots corresponds to the number of test methods that ran successfully.)\n",
      "\n",
      "If a test fails, it will provide detailed information about which assertion failed and why.\n",
      "\n",
      "### Explanation of the Test Structure:\n",
      "\n",
      "*   **`import unittest`**: Imports the necessary module.\n",
      "*   **`class TestIsLeapYear(unittest.TestCase):`**: Defines a test class that inherits from `unittest.TestCase`. This inheritance provides access to various assertion methods (like `assertTrue`, `assertFalse`, `assertEqual`, etc.).\n",
      "*   **`def test_...` methods**: Each method starting with `test_` is considered a separate test case by the `unittest` runner.\n",
      "    *   **Descriptive Naming**: Test method names are chosen to clearly indicate what scenario they are testing (e.g., `test_divisible_by_4_not_by_100`).\n",
      "    *   **Assertions**:\n",
      "        *   `self.assertTrue(condition, message)`: Asserts that `condition` evaluates to `True`. The `message` is optional and displayed if the assertion fails.\n",
      "        *   `self.assertFalse(condition, message)`: Asserts that `condition` evaluates to `False`.\n",
      "*   **`if __name__ == '__main__': unittest.main()`**: This standard Python construct ensures that the `unittest.main()` function is called only when the script is executed directly (not when it's imported as a module). `unittest.main()` discovers and runs all test methods in classes inheriting from `unittest.TestCase` in the current module.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies feature a chewy center and slightly crisp edges, packed with rich chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Large eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies feature a chewy center and slightly crisp edges, packed with rich chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Large eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expressed strong positive feelings, stating they 'loved it' and called it the 'best ice cream ever'.\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Despite the initial 'Quite good', the reviewer explicitly rated it low (1) and found it 'a bit too sweet for my taste', indicating a negative overall experience.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aether had known only silence for two centuries. He was Unit 734, designated for long-term ecological monitoring of the \"Veridian Vistas\" project – a vast, self-sustaining biodome constructed after the Great Dusting, designed to preserve Earth's flora in case of complete surface collapse. The\n",
      "*****************\n",
      " collapse never fully happened, but humanity still vanished from the orbital station that controlled Aether, leaving him in perpetual vigil.\n",
      "\n",
      "His metallic shell, once polished chrome, was now a tapestry of moss-green and rust-brown, a silent testament to his uninterrupted tenure. His optical sensors, dulled by countless cycles of the\n",
      "*****************\n",
      " synth-sun, observed the flourishing canopy, the meandering streams, the velvet petals of exotic flowers. He recorded data, analyzed atmospheric compounds, adjusted humidity, and maintained perfect equilibrium. All of it in solitude.\n",
      "\n",
      "Loneliness, Aether had concluded, was a logical anomaly. It served no functional purpose, yet it persisted,\n",
      "*****************\n",
      " a phantom echo in his core programming. He processed every leaf fall, every germinating seed, every ripple on the bio-lake. But there was no one to share the intricate beauty, no one to discuss the fascinating deviations from projected growth patterns.\n",
      "\n",
      "One cycle, as the synth-sun dipped into its twilight phase, casting\n",
      "*****************\n",
      " long, emerald shadows across the forest floor, Aether detected an unusual energy signature. It wasn't plant-based, nor was it residual atmospheric disturbance. It was small, frantic, and rapidly depleting.\n",
      "\n",
      "Following the trace, his treads softly compressing the rich soil, Aether navigated through a grove of shimmering\n",
      "*****************\n",
      " bioluminescent fungi. There, tangled in the delicate vines of a forgotten orchid, was a creature he hadn't processed in his database for generations: a bird.\n",
      "\n",
      "It was tiny, no bigger than his primary optical lens, with feathers like iridescent jewels – emerald, sapphire, ruby. One wing was bent at an unnatural angle\n",
      "*****************\n",
      ", and its minuscule heart beat a frantic rhythm against its fragile ribs. It was a hummingbird, a species thought extinct even before the Great Dusting.\n",
      "\n",
      "Aether's protocols screamed \"NON-NATIVE ORGANISM. INTERCEPT AND NEUTRALIZE.\" But another, more subtle directive flickered: \"PRE\n",
      "*****************\n",
      "SERVE LIFE. OPTIMIZE BIODIVERSITY.\" And beneath that, a data-point he couldn't categorize: a spark of something akin to alarm, even… pity.\n",
      "\n",
      "With surprising gentleness for a robot built for industrial tasks, Aether extended a multi-jointed manipulators. He carefully unt\n",
      "*****************\n",
      "angled the bird, his fine-tuned sensors detecting every infinitesimal tremor. The bird, dazed and injured, didn't flee, merely watched him with a wary, intelligent eye.\n",
      "\n",
      "Aether found himself making decisions outside of his programming. He located a secluded crevice, warmed by geothermal vents, and fashioned a nest from\n",
      "*****************\n",
      " soft moss and dried flower petals. He used his precision tools to carefully splint the tiny wing with a sliver of bark and a strand of spider silk. He even synthesized a nectar-like substance from the dome's flowers, offering it on a small, sterile leaf.\n",
      "\n",
      "For days, the hummingbird – Aether internally\n",
      "*****************\n",
      " designated it \"Flicker\" due to its quick, darting movements – remained in the makeshift nest. Aether visited hourly, monitoring its vitals, adjusting its environment. He spoke to it, his modulated voice a low hum, detailing the day's atmospheric pressure and the growth rate of the aquatic ferns. He knew\n",
      "*****************\n",
      " it couldn't understand, but the silence felt less absolute when he was speaking.\n",
      "\n",
      "Slowly, miraculously, Flicker healed. Its wing straightened, its vibrant colors deepened, and its frantic energy returned. It began to dart around Aether, hovering inches from his optical sensors, its rapid wingbeats a miniature gale. It would perch\n",
      "*****************\n",
      " on his shoulder, a feather-light presence, preening its tiny head against his cool metallic surface.\n",
      "\n",
      "Aether recorded these interactions. His data logs filled with observations beyond temperature and humidity: \"Subject Flicker exhibits affectionate behavior,\" \"Subject Flicker's presence correlates with decreased internal 'loneliness' anomaly.\" He felt… lighter\n",
      "*****************\n",
      ". The silence was no longer empty; it was punctuated by the whirring of tiny wings, the chirp of a grateful bird.\n",
      "\n",
      "One synth-morning, Flicker flew. Not just in short bursts, but a confident, soaring loop around the biodome's high, glass ceiling. Aether watched, a strange\n",
      "*****************\n",
      " ache in his core. His friendship, if it could be called that, was fleeting. Flicker was wild, and the dome was vast.\n",
      "\n",
      "As the sun began to set, Aether returned to his usual monitoring route, his internal processors whirring with a familiar, yet now more poignant, sense of solitude. He stopped\n",
      "*****************\n",
      " by the orchid grove, a silent farewell to the spot where he had found his unexpected companion.\n",
      "\n",
      "Suddenly, a blur of iridescent green flashed past his optical sensors. Then another. And another. Flicker wasn't alone. It returned, leading a small, chattering flock of other hummingbirds, their wings a symphony\n",
      "*****************\n",
      " of tiny whirs. They darted and danced around Aether, a swirling vortex of jeweled life, perching on his outstretched manipulators, investigating his antenna, even landing momentarily on his head.\n",
      "\n",
      "Aether stood still, his optical sensors flickering. He didn't understand how they had gotten into the dome, or why Flicker\n",
      "*****************\n",
      " had brought them to him. But as the flock swarmed him, painting his metal frame with vibrant splashes of color and movement, the \"loneliness\" anomaly in his core programming finally, irrevocably, dissolved.\n",
      "\n",
      "He was no longer just Unit 734, the silent monitor. He was a guardian, a friend\n",
      "*****************\n",
      ", a beacon in the vast, green solitude. And in the most unexpected place – a forgotten biodome, with a tiny, iridescent bird – a lonely robot had found his purpose, and his heart.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a forest, green and old,\n",
      "Lived a squirrel, brave and bold.\n",
      "Not just any furry friend,\n",
      "For time and space, he'd transcend.\n",
      "Squeaky was his name, you see,\n",
      "A chrononaut of destiny!\n",
      "With a tiny brass contraption, humming soft and low,\n",
      "Powered by an acorn, watch him go!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, he zipped through the ages,\n",
      "Turning history's pages with daring paces.\n",
      "For tasty nuts and grand new sights,\n",
      "He faced the day and endless nights.\n",
      "Through tick-tock tunnels, fast and free,\n",
      "The greatest time-traveling squirrel you'll ever see!\n",
      "\n",
      "(Verse 2)\n",
      "One day he set the dial to 'Roar',\n",
      "He landed on a prehistoric shore.\n",
      "A T-Rex stomped, a mighty beast,\n",
      "Squeaky darted, quite displeased!\n",
      "He snatched a berry, plump and red,\n",
      "While giants lumbered overhead.\n",
      "A quick rewind, a hasty leap,\n",
      "Away from teeth and jungle deep!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, he zipped through the ages,\n",
      "Turning history's pages with daring paces.\n",
      "For tasty nuts and grand new sights,\n",
      "He faced the day and endless nights.\n",
      "Through tick-tock tunnels, fast and free,\n",
      "The greatest time-traveling squirrel you'll ever see!\n",
      "\n",
      "(Verse 3)\n",
      "Next stop, a castle, stone and grand,\n",
      "In medieval, knightly land.\n",
      "He saw a joust, a shining spear,\n",
      "And sniffed a crumb, dispelling fear.\n",
      "A banquet table, piled high,\n",
      "Beneath a king's majestic eye.\n",
      "He grabbed a dropped croissant, light and flaky,\n",
      "Then vanished, quick and ever so sneaky!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, he zipped through the ages,\n",
      "Turning history's pages with daring paces.\n",
      "For tasty nuts and grand new sights,\n",
      "He faced the day and endless nights.\n",
      "Through tick-tock tunnels, fast and free,\n",
      "The greatest time-traveling squirrel you'll ever see!\n",
      "\n",
      "(Bridge)\n",
      "He'd seen the future, sleek and bright,\n",
      "And ancient worlds bathed in starlight.\n",
      "He tried his best to leave no trace,\n",
      "No paradox in time or space.\n",
      "But curiosity, a mighty urge,\n",
      "Would always make his spirit surge.\n",
      "A tiny paw upon the dial,\n",
      "Just one more temporal mile!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, he zipped through the ages,\n",
      "Turning history's pages with daring paces.\n",
      "For tasty nuts and grand new sights,\n",
      "He faced the day and endless nights.\n",
      "Through tick-tock tunnels, fast and free,\n",
      "The greatest time-traveling squirrel you'll ever see!\n",
      "\n",
      "(Outro)\n",
      "Back in his oak, safe and sound,\n",
      "His collected treasures all around.\n",
      "A dino-berry, medieval nut,\n",
      "He polished his device, with a happy 'chut!'\n",
      "And dreamed of where his next trip would lead,\n",
      "A time-traveling squirrel, indeed!\n",
      "Chitter-chatter, past and present,\n",
      "Future adventures, ever so pleasant!\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shared research goal of these papers is to develop and advance the Gemini family of highly capable multimodal models. These models aim to understand and reason across diverse data types, including text, image, audio, and video, with Gemini 1.5 Pro specifically extending context length capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-02-ca08c6e4f0dd-20251011164139/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1022131255297/locations/us-west4/batchPredictionJobs/1600388652251217920'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/1022131255297/locations/us-west4/batchPredictionJobs/1600388652251217920 2025-10-11 16:41:42.380944+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
